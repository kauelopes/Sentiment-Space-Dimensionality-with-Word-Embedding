{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "#[0]\n",
    "import numpy as np\n",
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "from gensim.test.utils import datapath, get_tmpfile\n",
    "import json #https://stackoverflow.com/questions/7100125/storing-python-dictionaries\n",
    "import pickle #https://stackoverflow.com/questions/11218477/how-can-i-use-pickle-to-save-a-dict\n",
    "import os\n",
    "#[1]\n",
    "from sklearn.manifold import MDS\n",
    "from scipy.spatial import procrustes\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "#[2]\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.cluster import DBSCAN\n",
    "\n",
    "#[3]\n",
    "from sklearn import decomposition\n",
    "from scipy.spatial import distance_matrix\n",
    "\n",
    "\n",
    "from sklearn.manifold import SpectralEmbedding\n",
    "from sklearn.manifold import Isomap\n",
    "\n",
    "from mpl_toolkits import mplot3d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = get_clean_embeddings()[0:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb = embeddings[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnd = np.zeros_like(emb)\n",
    "rnd2 = np.array(embeddings[1]).copy()\n",
    "rnd3 = np.array(emb).copy()\n",
    "\n",
    "for i in range(50):\n",
    "    a = np.random.randint(len(rnd3))\n",
    "    b = np.random.randint(len(rnd3))\n",
    "    tmp = rnd3[a]\n",
    "    rnd3[a] = rnd3[b]\n",
    "    rnd3[b] = tmp\n",
    "for i in range(25):\n",
    "    a = np.random.randint(len(rnd3))\n",
    "    b = np.random.randint(len(rnd3))\n",
    "    tmp = rnd2[a]\n",
    "    rnd2[a] = rnd2[b]\n",
    "    rnd2[b] = tmp\n",
    "    \n",
    "tmp = emb.reshape((len(emb)*len(emb[0])))\n",
    "\n",
    "\n",
    "for i in range(len(rnd)):\n",
    "    for j in range(len(rnd[0])):\n",
    "        rnd[i][j] = tmp[np.random.randint(len(tmp))]\n",
    "\n",
    "embeddings += [rnd2,rnd3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "for a in range(len(embeddings)):\n",
    "    for b in range(len(embeddings)):\n",
    "        embeddings[a],embeddings[b],r = procrustes(embeddings[a],embeddings[b])\n",
    "for a in range(len(embeddings)):\n",
    "    for b in range(len(embeddings)):\n",
    "        embeddings[0],embeddings[b],r = procrustes(embeddings[0],embeddings[b])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "resp = []\n",
    "for a in embeddings:\n",
    "    tmp = []\n",
    "    for b in embeddings:\n",
    "        tmp += [np.round(calcula_stress_embeddings(a,b),3)]\n",
    "    resp += [tmp]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0.0, 0.013, 0.025, 0.055, 0.095],\n",
       " [0.013, 0.0, 0.02, 0.052, 0.092],\n",
       " [0.026, 0.02, 0.0, 0.052, 0.09],\n",
       " [0.068, 0.063, 0.061, 0.0, 0.07],\n",
       " [0.144, 0.137, 0.131, 0.087, 0.0]]"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_emotion_embedding():\n",
    "    return get_emb_concat()\n",
    "\n",
    "def get_final_embs(dimensions = 200):\n",
    "    embs = get_clean_embeddings()\n",
    "    embs += [get_emb_soma()]\n",
    "    embs += [get_emb_concat()]\n",
    "\n",
    "\n",
    "    mds = MDS(n_components=dimensions)\n",
    "    #Trata embeddings, filtrando somente os sentimentos principais e colocando no numero minimo de dimensoes\n",
    "    embeddings = []\n",
    "    for i in range(len(embs)):\n",
    "        transformed_embedding = mds.fit_transform(embs[i])\n",
    "        embeddings += [transformed_embedding]\n",
    "    embs = embeddings\n",
    "\n",
    "    for a in range(len(embs)):\n",
    "        for b in range(len(embs)):\n",
    "            embs[a],embs[b],r = procrustes(embs[a],embs[b])\n",
    "    for i in range(200):\n",
    "        a = np.random.randint(len(embs))\n",
    "        b = np.random.randint(len(embs))\n",
    "        embs[a],embs[b],r = procrustes(embs[a],embs[b])\n",
    "    return embs\n",
    "\n",
    "def calcula_stress_embeddings(d_original, d_proposta):\n",
    "    d_original = distance_matrix(d_original,d_original)\n",
    "    d_proposta = distance_matrix(d_proposta,d_proposta)\n",
    "    a = np.sum((d_original-d_proposta)**2)\n",
    "    b = np.sum(d_original**2)\n",
    "    return np.sqrt(a/b)/4\n",
    "\n",
    "def calcula_stress(d_original, d_proposta):\n",
    "    a = np.sum((d_original-d_proposta)**2)\n",
    "    b = np.sum(d_original**2)\n",
    "    return np.sqrt(a/b)/4\n",
    "\n",
    "def get_emb_soma():\n",
    "    embeddings = get_clean_embeddings()\n",
    "    sentiments = get_sentiments()\n",
    "    assemble_embeddings_soma = np.zeros_like(embeddings[0])\n",
    "    for i in embeddings:\n",
    "        for c in range(75):\n",
    "            assemble_embeddings_soma[c] = assemble_embeddings_soma[c] + i[c]\n",
    "    return assemble_embeddings_soma\n",
    "\n",
    "def get_emb_concat():\n",
    "    embeddings = get_clean_embeddings()\n",
    "    sentiments = get_sentiments()\n",
    "    assemble_embeddings_concat = []\n",
    "\n",
    "    for c in range(len(sentiments)):\n",
    "        tmp = np.array([])\n",
    "        for i in embeddings:\n",
    "            tmp = np.concatenate((tmp,i[c]))\n",
    "        assemble_embeddings_concat += [tmp]\n",
    "    return assemble_embeddings_concat\n",
    "\n",
    "\n",
    "def get_emb_media():\n",
    "    #média dos vetores e retirando outliers\n",
    "    embeddings = get_clean_embeddings()\n",
    "    sentiments = get_sentiments()\n",
    "    assemble_embeddings_media = np.zeros_like(embeddings[0])\n",
    "    vec = []\n",
    "    dist = []\n",
    "    for c in range(75):\n",
    "        tmp = np.array([])\n",
    "        vec = []\n",
    "        for i in embeddings:\n",
    "            vec += [i[c]]\n",
    "\n",
    "        centroid = np.zeros_like(vec[0])\n",
    "        for i in vec:\n",
    "            centroid = centroid + i\n",
    "        centroid = centroid/len(vec)\n",
    "\n",
    "        dist = []\n",
    "        for i in vec:\n",
    "            dist += [np.linalg.norm(i-centroid)]\n",
    "\n",
    "        limitante_distancia_para_corte = np.max(dist)\n",
    "\n",
    "        counter = 0\n",
    "        for n,i in enumerate(embeddings):\n",
    "            if dist[n]!=limitante_distancia_para_corte:\n",
    "                assemble_embeddings_media[c] += i[c] \n",
    "                counter+=1\n",
    "\n",
    "        assemble_embeddings_media[c] = assemble_embeddings_media[c]/counter\n",
    "    return assemble_embeddings_media\n",
    "\n",
    "\n",
    "def get_clean_embeddings():\n",
    "#Seleciona somente os vetores principais das nossas embeddings\n",
    "    sentiments = get_sentiments()\n",
    "    raw_embeddings = []\n",
    "    dimensions_size = []\n",
    "    for i in range(4):\n",
    "        tmp = get_vectors_embedding(i)\n",
    "        raw_embeddings += [tmp]\n",
    "        dimensions_size+=[len(get_sentiment_vector(tmp,sentiments[0]))]\n",
    "#Faz o corte para o menor tamanho de dimensão\n",
    "    n_dimensions = np.min(dimensions_size)\n",
    "    # n_dimensions = 10\n",
    "    mds = MDS(n_components=n_dimensions)\n",
    "#Trata embeddings, filtrando somente os sentimentos principais e colocando no numero minimo de dimensoes\n",
    "    embeddings = []\n",
    "    for i in range(4):\n",
    "        tmp_embedding = []\n",
    "        for s in sentiments:\n",
    "            tmp_embedding += [get_sentiment_vector(raw_embeddings[i],s)]\n",
    "        transformed_embedding = mds.fit_transform(tmp_embedding)\n",
    "        embeddings += [transformed_embedding]\n",
    "    # Calcula erro entre nossos modelos\n",
    "    for a in range(len(embeddings)):\n",
    "        for b in range(len(embeddings)):\n",
    "            embeddings[a],embeddings[b],r = procrustes(embeddings[a],embeddings[b])\n",
    "    return embeddings\n",
    "\n",
    "def print_anotado(emb):\n",
    "    fig, ax = plt.subplots(figsize=(15,15))\n",
    "    ax.scatter(emb[:,0], emb[:,1])\n",
    "    n = get_sentiments()\n",
    "    \n",
    "    for i, txt in enumerate(n):\n",
    "        ax.annotate(\" \"+txt, (emb[:,0][i], emb[:,1][i]))\n",
    "\n",
    "def print_anotado2(emb):\n",
    "    fig, ax = plt.subplots(figsize=(15,15))\n",
    "    for e in emb:\n",
    "        ax.scatter(e[:,0], e[:,1])\n",
    "        n = get_sentiments()\n",
    "\n",
    "        for i, txt in enumerate(n):\n",
    "            ax.annotate(txt, (e[:,0][i], e[:,1][i]))\n",
    "\n",
    "def print_2d(embs):\n",
    "    res = []\n",
    "    mds_2 = MDS(n_components=2)\n",
    "    for e in embs:\n",
    "        res += [mds_2.fit_transform(e)]\n",
    "    for p in res:\n",
    "        plt.scatter(p[:,0],p[:,1])\n",
    "\n",
    "def get_sentiment_vector(embedding, sentiment):\n",
    "    return embedding[sentiment][\"vectors\"][sentiment]\n",
    "\n",
    "def get_vectors_embedding(n):\n",
    "    files = os.listdir(\"models\")\n",
    "    model_file_name = []\n",
    "    for file in files:\n",
    "        model_file_name += [file]\n",
    "    escolhidos = [model_file_name[14],model_file_name[5],model_file_name[7],model_file_name[3]]\n",
    "    with open(\"models/\"+escolhidos[n], 'rb') as handle:\n",
    "        b = pickle.load(handle)\n",
    "    return b\n",
    "\n",
    "def get_sentiments_old():\n",
    "    sentiments = np.load(\"./sentiments_list.npy\")\n",
    "    return sentiments\n",
    "def get_sentiments():\n",
    "    sentiments = np.load(\"./sentiments_list.npy\")\n",
    "    sentiments = np.concatenate((sentiments[:26],sentiments[27:]))\n",
    "    return sentiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = os.listdir(\"models\")\n",
    "model_file_name = []\n",
    "for file in files:\n",
    "    model_file_name += [file]\n",
    "escolhidos = [model_file_name[14],model_file_name[5],model_file_name[7],model_file_name[3]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['wiki-news-300d-1M.vec.pickle',\n",
       " 'crawl-300d-2M.vec.pickle',\n",
       " 'glove.840B.300d.txt.pickle',\n",
       " 'glove.6B.300d.txt.pickle']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "escolhidos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
